{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_LSTM(3)","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMCVZ/Qky0qrocyRwHuOMtp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ScCTvLvBTMkt","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599056602942,"user_tz":-480,"elapsed":2037,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"8fe652ff-9fe4-41f0-dffe-9c32edecbc6e"},"source":["from google.colab import drive\n","drive.mount('/content/drive') "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BaR9_4wkyB0d"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","from tqdm import tqdm \n","### BUG：混合了Tensorflow keras和keras API。优化器和模型应来自同一层定义。 \n","from scipy.signal import resample \n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf \n","from tensorflow.keras import backend as K\n","from tensorflow.keras import Input, Model, regularizers\n","from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n","# tensorflow 2.0没有加入对CuDNNLSTM的支持。但是，tensorflow的LSTM对GPU加速优化的稀烂，专门引入只能由GPU加速并行运算的CuDNNLSSTM（）\n","# 把LSTM改成CuDNNLSTM之后，训练速度至少提升了5倍以上\n","# CuDNNLSTM是为CUDA并行处理而设计的，如果没有GPU，它将无法运行。而LSTM是为普通CPU设计的。由于并行性，执行时间更快。\n","from tensorflow.keras.layers import Concatenate, Reshape, GlobalAveragePooling1D, GaussianNoise, Bidirectional, Layer, Conv1D, BatchNormalization, MaxPooling1D, AveragePooling1D, LayerNormalization, Dropout, Dense, concatenate, Activation, Lambda, dot, Multiply\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","### 注意：为了防止不必要的BUG，用keras的时候，必须要带上tensorflow，即tensorflow.keras，遇到的BUG：元组对象没有layer属性，\n","### 另外，只有keras可能导致调用的东西无效。比如，ReduceLROnPlateau在用keras.callbacks中import的时候，没有起作用，从tensorflow.keras.callbacks调用有作用。\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight ## 计算样本类别占比，判断是否平衡\n","### 执行下面语句能够尽量减少每次keras分数的不确定 ###\n","SEED = 42\n","import random\n","import gc\n","import os\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","######################################################\n","data_path = '/content/drive/My Drive/kesci_JZB/交子杯算法赛道—渣渣炼丹师队/code/data/'\n","model_path = '/content/drive/My Drive/kesci_JZB/交子杯算法赛道—渣渣炼丹师队/code/model/'\n","npy_path = '/content/drive/My Drive/kesci_JZB/交子杯算法赛道—渣渣炼丹师队/code/npy_file/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5PgdSSqyByO"},"source":["def acc_combo(y, y_pred, mode):\n","    # 数值ID与行为编码的对应关系\n","  mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', 4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n","          8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', 12: 'C_1', 13: 'C_3', 14: 'C_0', # 递手机\n","          15: 'B_6', 16: 'C_2', 17: 'C_5', 18: 'C_6' }\n","  if mode == 'behavior':  # 场景+动作\n","    code_y, code_y_pred = mapping[y], mapping[y_pred] \n","    if code_y == code_y_pred: # 编码完全相同得分1.0 即 C_0 == C_0\n","      return 1.0\n","    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: # 场景相同得 1.0/7 分\n","      return 1.0/7\n","    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: # 动作相同得 1.0/3 分\n","      return 1.0/3\n","    else: # 都不对，不得分\n","      return 0.0 \n","\n","# 自定义评估函数：get_acc_combo()\n","def get_acc_combo():\n","    def combo(y, y_pred):\n","        # 数值ID与行为编码的对应关系\n","        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3',\n","                4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5',\n","                8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6',\n","                12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6',\n","                16: 'C_2', 17: 'C_5', 18: 'C_6'}\n","        # 将行为ID转为编码\n","\n","        code_y, code_y_pred = mapping[int(y)], mapping[int(y_pred)]\n","        if code_y == code_y_pred: #编码完全相同得分1.0\n","            return 1.0\n","        elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n","            return 1.0/7\n","        elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n","            return 1.0/3\n","        else:\n","            return 0.0\n","    confusionMatrix=np.zeros((19,19))\n","    for i in range(19):\n","      for j in range(19):\n","        confusionMatrix[i,j]=combo(i,j)\n","    confusionMatrix=tf.convert_to_tensor(confusionMatrix)\n","\n","    def acc_combo(y, y_pred):\n","      y=tf.argmax(y,axis=1)\n","      y_pred = tf.argmax(y_pred, axis=1)\n","      indices=tf.stack([y,y_pred],axis=1)\n","      scores=tf.gather_nd(confusionMatrix,tf.cast(indices,tf.int32))\n","      return tf.reduce_mean(scores)\n","    return acc_combo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0IsSqzxyBqE","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"ok","timestamp":1599021873268,"user_tz":-480,"elapsed":21456,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"47163589-1cab-4d83-dfc4-6372f5a5617f"},"source":["train = pd.read_csv(data_path + 'sensor_train.csv')\n","test = pd.read_csv(data_path + 'sensor_test.csv')\n","sub = pd.read_csv(data_path + 'result.csv')\n","\n","y = train.groupby('fragment_id')['behavior_id'].min()\n","\n","def add_features(df):\n","  print(df.columns)\n","\n","  df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n","  df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n","  # 水平倾斜角(角度)\n","  df['thetax']=np.arctan(df.acc_xg / np.sqrt(df.acc_yg*df.acc_yg+df.acc_zg*df.acc_zg)) * 180 / np.pi\n","  df['thetay']=np.arctan(df.acc_yg / np.sqrt(df.acc_xg*df.acc_xg+df.acc_zg*df.acc_zg)) * 180 / np.pi\n","  df['thetaz']=np.arctan(df.acc_zg / np.sqrt(df.acc_yg*df.acc_yg+df.acc_xg*df.acc_xg)) * 180 / np.pi\n","\n","  df['xy'] = (df['acc_x'] ** 2 + df['acc_y'] ** 2) ** 0.5\n","  df['xy_g'] = (df['acc_xg'] ** 2 + df['acc_yg'] ** 2) ** 0.5 \n","\n","  df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n","  print(df.columns)\n","  return df\n","\n","train = add_features(train)\n","test = add_features(test)\n","\n","## 8个分别一阶差分\n","train_diff1 = pd.DataFrame()\n","diff_fea = ['acc_x','acc_y','acc_z','acc_xg','acc_yg','acc_zg','acc','accg','thetax','thetay','thetaz','xy','xy_g','g']\n","train_diff1 = train.groupby('fragment_id')[diff_fea].diff(1).fillna(0.) \n","train_diff1.columns = ['x_diff_1','y_diff_1','z_diff_1','xg_diff_1','yg_diff_1','zg_diff_1','acc_diff_1',\n","             'accg_diff_1','thetax_diff_1','thetay_diff_1','thetaz_diff_1','xy_diff_1','xy_g_diff_1','g_diff_1']\n","\n","test_diff1 = pd.DataFrame()\n","test_diff1 = test.groupby('fragment_id')[diff_fea].diff(1).fillna(0.)\n","test_diff1.columns = train_diff1.columns\n","## 8个分别二阶差分\n","train_diff2 = pd.DataFrame()\n","train_diff2 = train.groupby('fragment_id')[diff_fea].diff(2).fillna(0.) \n","train_diff2.columns = ['x_diff_2','y_diff_2','z_diff_2','xg_diff_2','yg_diff_2','zg_diff_2','acc_diff_2',\n","             'accg_diff_2','thetax_diff_2','thetay_diff_2','thetaz_diff_2','xy_diff_2','xy_g_diff_2','g_diff_2']\n","\n","test_diff2 = pd.DataFrame()\n","test_diff2 = test.groupby('fragment_id')[diff_fea].diff(2).fillna(0.)\n","test_diff2.columns = train_diff2.columns\n","\n","## 融合\n","train = pd.concat([train, train_diff1, train_diff2], axis = 1)\n","test = pd.concat([test, test_diff1, test_diff2], axis = 1)\n","\n","No_train_fea = ['fragment_id', 'time_point', 'behavior_id', 'train_scene', 'train_action']\n","train_fea = [fea for fea in train.columns if fea not in No_train_fea]\n","fea_num = len(train_fea)\n","## 归一化\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler().fit(train[train_fea])\n","\n","train[train_fea] = pd.DataFrame(scaler.transform(train[train_fea]), columns = train_fea)\n","test[train_fea] = pd.DataFrame(scaler.transform(test[train_fea]), columns = train_fea)\n","'''\n","分析：\n","如果某个特征的方差远大于其它特征的方差，那么它将会在算法学习中占据主导位置，导致我们的学习器不能像我们期望的那样，去学习其他的特征，\n","这将导致最后的模型收敛速度慢甚至不收敛，因此需要对这样的特征数据进行标准化/归一化。转化函数为：x =(x - 𝜇)/𝜎\n","''' "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg', 'behavior_id'],\n","      dtype='object')\n","Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'g'],\n","      dtype='object')\n","Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg'],\n","      dtype='object')\n","Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg', 'acc', 'accg', 'g'],\n","      dtype='object')\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n分析：\\n如果某个特征的方差远大于其它特征的方差，那么它将会在算法学习中占据主导位置，导致我们的学习器不能像我们期望的那样，去学习其他的特征，\\n这将导致最后的模型收敛速度慢甚至不收敛，因此需要对这样的特征数据进行标准化/归一化。转化函数为：x =(x - 𝜇)/𝜎\\n'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"k3cML0qGyBfd","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1599021921234,"user_tz":-480,"elapsed":44417,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"7a0bf970-7e41-4b3a-92fe-d522e0309dc0"},"source":["## 原\n","x = np.zeros((7292, 60, fea_num)) \n","t = np.zeros((7500, 60, fea_num))\n","## 先采集原来的数据集 \n","for i in tqdm(range(7292)):\n","  tmp = train[train.fragment_id == i][:60].reset_index(drop = True) \n","  x[i, :, :] = resample(tmp[train_fea], 60, np.array(tmp.time_point))[0]\n","for i in tqdm(range(7500)):\n","  tmp = test[test.fragment_id == i][:60].reset_index(drop = True)\n","  t[i, :, :] = resample(tmp[train_fea], 60, np.array(tmp.time_point))[0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 7292/7292 [00:21<00:00, 336.20it/s]\n","100%|██████████| 7500/7500 [00:21<00:00, 342.30it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GIW1W3OvfSZL"},"source":["import albumentations as A \n","# 图像中生成正方形黑块，参考：https://albumentations.readthedocs.io/en/latest/api/augmentations.html?highlight=cutout#albumentations.augmentations.transforms.Cutout\n","def data_Cutout(x, y): \n","    x1 = np.zeros((x.shape[0], x.shape[1], x.shape[2]))\n","    for i in range(x.shape[0]):\n","      transform = A.Cutout(num_holes=8, max_h_size=1, max_w_size=1, always_apply=False, p=0.5) # Cutout 剪下8个正方形的小黑块\n","      x1[i, :, :] = transform(image = x[i, :, :])['image'] # 生成一个裁剪之后的图片\n","    x = np.vstack((x, x1)) # 多维数组垂直堆叠\n","    y = np.hstack((y, y)) # y为标签，一维数组，可以水平堆叠，注意：标签还是原来的标签。\n","    return x, y "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p_oZLDM5Oe0","colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"status":"ok","timestamp":1599021928172,"user_tz":-480,"elapsed":48725,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"866d65f1-d876-4b38-fc4e-1869c81c960c"},"source":["!pip install keras-self-attention \n","from keras_self_attention import SeqSelfAttention"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras-self-attention\n","  Downloading https://files.pythonhosted.org/packages/39/0d/b8ab8469ae55cea199574f4d2c30da4656d310a833a67bb422ad8a056bf0/keras-self-attention-0.47.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.4.3)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.47.0-cp36-none-any.whl size=17289 sha256=2f3d379e5cbd9376f15cfed176418cb7decfa3a59f97573e00aaede2aae1a42b\n","  Stored in directory: /root/.cache/pip/wheels/70/87/01/76c703d5401b65e323927c1fdc665f3fb143282ff67d71e859\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.47.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e2a_F99B5n7j","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599037576828,"user_tz":-480,"elapsed":3970,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"7b34424a-5977-4acc-f9c7-ef42cfc6c50d"},"source":["# # 在colab上加载本地的.py文件\n","# from google.colab import files\n","# src = list(files.upload().values())[0]\n","# open('cos_dense_attention.py','wb').write(src) \n","\n","# import os \n","# os.chdir('/content/drive/My Drive/kesci_JZB/交子杯算法赛道—渣渣炼丹师队/code/Codes')\n","\n","%cd '/content/drive/My Drive/kesci_JZB/交子杯算法赛道—渣渣炼丹师队/code/Codes'\n","# 在colab中使用命令需要注意对空格的转义，比如!python /content/drive/My\\ Drive/BertNer/BERT_NER.py中的My\\ Drive\n","from cos_dense_attention import Attention "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/kesci_JZB/交子杯算法赛道—渣渣炼丹师队/code/Codes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pYIxNO6-zHXV"},"source":["def Net(): \n","  input = Input(shape=(60, fea_num)) \n","  # model = GaussianNoise(0.1)(input) # 先给数据加入高斯噪声进行数据增强，防止过拟合  为数据施加0均值，标准差为stddev的加性高斯噪声\n","  model = Conv1D(512, kernel_size, input_shape=(60, fea_num), padding='same', kernel_regularizer = regularizers.l2(0.01))(input) \n","  model = BatchNormalization()(model) # BN在RNN等动态网络和batchsize较小的时候效果不好。\n","  model = Activation(activation=act_swish)(model) \n","  model = squeeze_excitation_layer(model, 512) ## SEnet --> 有提升 \n","  ### 总结：为了activation能更有效地使用输入信息，所以一般放在激活函数之前，但是对于relu在BN之前、之后差别不大 ###\n","  ### 应用中一般使用 tf.layers.batch_normalization 进行归一化操作。https://www.cnblogs.com/eilearn/p/9780696.html ###\n","  model = AveragePooling1D(pool_size=pool_size)(model) \n","  model = Dropout(0.4)(model) \n","  # 一般来说，Dropout仅在池化层后使用，另外，Conv1D的特征图数越多，dropout可以设置的越大 \n","  \n","  model = Conv1D(256, kernel_size, padding='same')(model) \n","  model = BatchNormalization()(model) \n","  model = Activation(activation=act_swish)(model)\n","  model = squeeze_excitation_layer(model, 256) \n","  model = AveragePooling1D(pool_size=pool_size)(model)  \n","  model = Dropout(0.4)(model)\n","\n","  model = Conv1D(128, kernel_size, padding='same')(model) \n","  model = BatchNormalization()(model) \n","  model = Activation(activation=act_swish)(model) \n","  model = squeeze_excitation_layer(model, 128)  \n","  model = AveragePooling1D(pool_size=pool_size)(model)\n","  model = Dropout(0.5)(model) \n","\n","  # 单向lstm\n","  model = CuDNNLSTM(64, return_sequences=True)(model) \n","  model = LayerNormalization()(model)\n","  model = Dropout(0.3)(model)\n","\n","  model = CuDNNLSTM(64, return_sequences=True)(model) \n","  model = LayerNormalization()(model)\n","  model = Dropout(0.3)(model)\n","  \n","  model = CuDNNLSTM(64, return_sequences=True)(model)\n","  model = LayerNormalization()(model)\n","  model = Dropout(0.4)(model)\n","\n","  model = CuDNNLSTM(19, return_sequences=True)(model) \n","  # model = SeqSelfAttention()(model) # (?, 7, 19) --> (?, ?, 19)\n","  model = Attention()([model, model]) ### 效果比上面要好一些。\n","  model = LayerNormalization()(model) # BatchNormalization\n","  model = Dropout(0.3)(model) \n","  model = GlobalAveragePooling1D()(model) # 加上SeqSelfAttention()需要有GlobalAveragePooling1D()，把(?, ?, 19)-->(?, 19) \n","  # model = Dropout(0.3)(model) \n","  # model = CuDNNGRU(150, return_sequences=True)(model) # (None,180)\n","  # model = attention_3d_block(model)\n","\n","# Trick：在卷积层之后，用GAP(Global Average Pooling)替代FC全连接层。\n","# 有两个好处：一是GAP在特征图与最终的分类间转换更加简单自然；二是不像FC层需要大量训练调优的参数，降低了空间参数会使模型更加健壮，抗过拟合效果更佳。\n","# 参考：http://spytensor.com/index.php/archives/19/?wefwpq=inmhj1\n","\n","  ## 双向lstm + attention\n","  # model = Bidirectional(LSTM(180, return_sequences=True))(model)  # 默认激活函数为tanh\n","  # model = Bidirectional(LSTM(180, return_sequences=True))(model)\n","  # model = Bidirectional(LSTM(180, return_sequences=True))(model)\n","  # model = attention_3d_block(model)\n","\n","  # model = Dense(n_classes)(model) \n","  # model = BatchNormalization()(model) \n","  output = Activation('softmax', name=\"softmax\")(model) \n","  '''\n","  Trick：使用GAP(GlobalAveragePooling)来替代最后的全连接层Dense(n_classes)，直接实现了降维，并且也极大地降低了网络参数(全连接层的参数在整个模型中的占比)，\n","  更重要的一点是保留了由前面各个卷积层和池化层提取到的空间信息，实际应用中效果提升也比较明显。另外，别忘了，还需要激活函数进行激活。\n","  '''\n","  return Model(input, output) \n","\n","############################ SENet ############################\n","def squeeze_excitation_layer(x, out_dim, ratio=16): # out_dim 跟前面的卷积层filters个数一样。\n","    '''\n","    SE module performs inter-channel weighting. 挤压激励模型执行通道间加权 \n","    中心思想：通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。\n","    SENet网络的创新点在于关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。\n","    SE模块是在channel维度上做attention或者gating操作，这种注意力机制让模型可以更加关注信息量最大的channel特征，而抑制那些不重要的channel特征.\n","    另外一点是SE模块是通用的，这意味着其可以嵌入到现有的网络架构中。\n","    参考：https://zhuanlan.zhihu.com/p/65459972\n","    '''\n","    # Squeeze操作得到全局描述特征\n","    squeeze = GlobalAveragePooling1D()(x) \n","    ''' \n","    x为3维输入，比如：卷积层的最后输出是h × w × d 的三维特征图，具体大小为6 × 6 × 3，经过GAP转换后，得到(1, 1, 3)的输出值，即\n","    特征图每一channel的h × w会被平均化成一个值，这个过程称为“squeeze”，GAP的使用一般在卷积层之后，输出层之前。\n","    参考：https://www.cnblogs.com/hutao722/p/10008581.html \n","    ''' \n","    # Excitation操作来抓取channel之间的关系 \n","    excitation = Dense(units = out_dim / ratio)(squeeze) # 第一个FC层起到降维的作用，采用ReLU激活\n","    excitation = BatchNormalization()(excitation)\n","    excitation = Activation('relu')(excitation) \n","    excitation = Dense(units = out_dim)(excitation) # 第二个FC层恢复原始的维度\n","    excitation = BatchNormalization()(excitation)\n","    excitation = Activation('hard_sigmoid')(excitation) # hard sigmoid具有非光滑性，与sigmoid相比，hard sigmoid的好处就是计算量小，不用求e的幂次\n","    excitation = Reshape((1, out_dim))(excitation) # 重塑为二维输出 out_dim为特征图的个数，而1代表一维，如果是二维的话，就是(1, 1, out_dim)\n","    scale = Multiply()([excitation, x]) # 该层接收一个列表的同shape张量，并返回它们的逐元素积的张量，shape不变。\n","    '''  \n","    最后将学习到的各个channel的激活值（sigmoid激活，值0~1）乘以x(对应的特征图)上的原始特征，\n","    其实整个操作可以看成学习到了各个channel的权重系数，从而使得模型对各个channel的特征更有辨别能力，这应该也算一种attention机制。\n","    '''\n","    return scale "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWvvVpWlYtFn"},"source":["# 统计代码运行的时间\n","from time import *\n","begin_time = time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUUn_86heFe7"},"source":["epochs = 500\n","batch_size = 128 \n","# 尝试：以128为分界线，向下（*0.5）和向上（*2）训练后比较测试结果，若向下更好则再*0.5，直接结果不再提升\n","# batchsize设置：通常10到100，一般设置为2的n次方。原因：计算机的gpu和cpu的memory都是2进制方式存储的，设置2的n次方可以加快计算速度。\n","kernel_size = 3 \n","pool_size = 2\n","dropout_rate = 0.4 # 防止过拟合\n","n_classes = 19 \n","cv = 20 \n","act_swish = lambda x:x * tf.nn.sigmoid(x) # Swish 是一种新型激活函数，公式为： f(x) = x · sigmoid(x)。Swish 具备无上界有下界、平滑、非单调的特性\n","# h_swish = lambda x:x * tf.nn.relu6(x + 3) / 6 # 虽然swish激活函数能够有效提高网络的精度，但是，swish的计算量太大，所以计算速度较慢。\n","proba_t = np.zeros((7500, n_classes)) \n","valid = np.zeros((7292, n_classes))\n","\n","cvscores = [] # 存储每折模型对验证集评估的分数\n","\n","# loss平滑标签\n","# CC_Ls = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.05) \n","###################### 类别不平衡分析 ######################\n","classweights=compute_class_weight(\"balanced\", np.array(range(19)), pd.read_csv(data_path+'sensor_train.csv')['behavior_id'])\n","classweights=pd.DataFrame(classweights)[0].to_dict() \n","print(classweights)\n","\n","kfold = StratifiedKFold(n_splits = cv, shuffle = True) \n","for fold, (xx, yy) in enumerate(kfold.split(x, y)): \n","  print('Processing fold: %d (%d, %d)' % (fold, len(xx), len(yy)))\n","\n","  y_ = to_categorical(y, num_classes=19) # 类别标签转换成二进制矩阵 \n","  x1, y1 = data_Cutout(x[xx], y[xx]) # 原数据(24/25 --> 7000) + 裁剪数据(7000)\n","  y1 = to_categorical(y1, num_classes=19)\n","\n","  model = Net()\n","  model.summary() # 查看训练参数\n","  model.compile(loss = 'CategoricalCrossentropy', optimizer = 'Nadam', metrics = ['acc']) # 函数的返回值为acc_combo get_acc_combo()\n","  graph_path = model_path + 'ConvLstm_model.png'\n","  plot_model(model, to_file = graph_path, show_shapes = True)  # 绘制模型图\n","  plateau = ReduceLROnPlateau(monitor = 'val_acc', verbose = 0, mode = 'max', factor = 0.1, patience = 8) # 'val_acc_combo'\n","  early_stopping = EarlyStopping(monitor = 'val_acc', verbose = 0, mode = 'max', patience = 30) # 防止过拟合\n","  checkpoint = ModelCheckpoint(model_path + f'foldCovLstm{fold}.h5', monitor = 'val_acc', verbose = 0, mode = 'max', save_best_only = True)\n","  # 单通道模型 \n","  history = model.fit( x1,y1,  \n","              epochs=epochs, \n","              batch_size=batch_size, \n","              verbose=1, \n","              shuffle=True,\n","              validation_data=(x[yy], y_[yy]), \n","              callbacks=[plateau, early_stopping, checkpoint], \n","              class_weight=classweights ### Trick：针对类别不平衡，设置class_weight，注意：classweights必须是字典\n","             ) ## 注：model.fit（）不返回Keras模型，而是一个History对象，其中包含训练的损失和度量值。\n","\n","  scores = model.evaluate(x[yy], y_[yy], verbose=0) # 用训练好的模型对验证集进行评估\n","  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100)) # model.metrics_names[1]为acc scores[0]为损失值、scores[1]为准确率\n","  cvscores.append(scores[1] * 100) \n","  model.load_weights(model_path + f'foldCovLstm{fold}.h5') # 用ModelCheckpoint保存了训练过程中的结果最好epoch的参数来预测新样本，load_weights()仅读取权重，但是需要在已有模型基础之上才可以调用\n","  \n","  ## 对于每一折的验证集都计算得分\n","  val = model.predict(x[yy], verbose=1, batch_size=1024) \n","  val_labels = np.argmax(val, axis=1) # 每一行最大概率的索引，即行为id\n","  val_score = sum(acc_combo(y_true, y_pred, 'behavior') for y_true, y_pred in zip(y[yy], val_labels)) / val_labels.shape[0]\n","\n","  print('官方得分：', round(val_score, 5)) # 保留小数点后五位 \n","  print('准确率得分：', round(accuracy_score(y[yy].values, val_labels), 5)) \n","\n","  valid[yy] += model.predict(x[yy], verbose=1, batch_size=1024)\n","  proba_t += model.predict(t, verbose=1, batch_size=1024) / 20. # 单模stacking (7500, 19)\n","\n","\n","  del model\n","  gc.collect()\n","\n","print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores))) # 对于%%，第一个%起到转义的作用，使结果输出百分号%\n","\n","# 计算执行时间 \n","end_time = time()\n","run_time = (end_time - begin_time) / 60\n","print('该循环程序运行时间：', run_time, '分钟') \n","\n","np.save(npy_path + 'lstm_valid.npy', valid)\n","np.save(npy_path + 'lstm_test.npy', proba_t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YGJw_pa6FOFQ"},"source":["## MTM LSTM模型"]},{"cell_type":"code","metadata":{"id":"b8eaQeiug-s7","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"ok","timestamp":1599057136041,"user_tz":-480,"elapsed":34121,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"0d241194-902d-42bf-89a6-6109dcd667c8"},"source":["train = pd.read_csv(data_path + 'sensor_train.csv')\n","test = pd.read_csv(data_path + 'sensor_test.csv')\n","sub = pd.read_csv(data_path + 'result.csv')\n","\n","y = train.groupby('fragment_id')['behavior_id'].min()\n","\n","def add_features(df):\n","  print(df.columns)\n","  df['acc'] = (df.acc_x ** 2 + df.acc_y ** 2 + df.acc_z ** 2) ** .5\n","  df['accg'] = (df.acc_xg ** 2 + df.acc_yg ** 2 + df.acc_zg ** 2) ** .5\n","  df['g'] = ((df[\"acc_x\"] - df[\"acc_xg\"]) ** 2 + (df[\"acc_y\"] - df[\"acc_yg\"]) ** 2 + (df[\"acc_z\"] - df[\"acc_zg\"]) ** 2) ** 0.5\n","  print(df.columns)\n","  return df\n","\n","train = add_features(train)\n","test = add_features(test)\n","\n","train_fea = [fea for fea in train.columns if fea not in ['fragment_id', 'time_point', 'behavior_id']]\n","fea_num = len(train_fea)\n","## 归一化 \n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler().fit(train[train_fea])\n","\n","train[train_fea] = pd.DataFrame(scaler.transform(train[train_fea]), columns = train_fea)\n","test[train_fea] = pd.DataFrame(scaler.transform(test[train_fea]), columns = train_fea)\n","\n","## 原\n","x = np.zeros((7292, 60, fea_num)) \n","t = np.zeros((7500, 60, fea_num))\n","## 先采集原来的数据集 \n","for i in tqdm(range(7292)):\n","  tmp = train[train.fragment_id == i][ : 60].reset_index(drop = True) \n","  x[i, :, :] = resample(tmp[train_fea], 60, np.array(tmp.time_point))[0]\n","for i in tqdm(range(7500)):\n","  tmp = test[test.fragment_id == i][:60].reset_index(drop = True)\n","  t[i, :, :] = resample(tmp[train_fea], 60, np.array(tmp.time_point))[0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg', 'behavior_id'],\n","      dtype='object')\n","Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg', 'behavior_id', 'acc', 'accg', 'g'],\n","      dtype='object')\n","Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg'],\n","      dtype='object')\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/7292 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Index(['fragment_id', 'time_point', 'acc_x', 'acc_y', 'acc_z', 'acc_xg',\n","       'acc_yg', 'acc_zg', 'acc', 'accg', 'g'],\n","      dtype='object')\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7292/7292 [00:16<00:00, 454.09it/s]\n","100%|██████████| 7500/7500 [00:16<00:00, 463.28it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4WS6tAbUFS66"},"source":["def LSTM_A(input, INPUT_SIZE=8, CELL_SIZE=64):\n","\n","    TIME_STEPS = 60\n","    x = CuDNNLSTM(CELL_SIZE, input_shape=(TIME_STEPS, INPUT_SIZE), return_sequences=True)(input)\n","    x = LayerNormalization()(x)  \n","    x = Dropout(0.5)(x) \n","    \n","    x = CuDNNLSTM(CELL_SIZE, return_sequences=True)(x)\n","    x = LayerNormalization()(x) \n","    x = Dropout(0.5)(x) \n","    \n","    x = CuDNNLSTM(CELL_SIZE, return_sequences=True)(x)\n","    x = Attention()([x,x])\n","    x = LayerNormalization()(x) \n","    x = Dropout(0.5)(x) \n","    \n","    x = GlobalAveragePooling1D()(x) # 类似于Dense   \n","    return x\n","\n","def LSTM_Model():\n","    \n","    TIME_STEPS = 60\n","    INPUT_SIZE = fea_num\n","    inputs = Input(shape=[TIME_STEPS, INPUT_SIZE]) \n","    part = tf.split(inputs, num_or_size_splits = [3,3,1,1,1], axis=2) # INPUT_SIZE axis=2 --> [3,3,1,1,1]\n","    # ''' \n","    # tf.split：将张量对特征分割成子张量，进行分别学习。\n","    # axis=2 对应的值为3+3+1+1+1=9，只在这个维度上拆成5份，分别为：(?, TIME_STEPS, 3),(?, TIME_STEPS, 3),(?, TIME_STEPS, 1),...,(?, TIME_STEPS, 1)\n","    # 后面再用Concatenate()对不同的特征张量进行分开训练，比如：Concatenate()([part[0],part[2]])\n","    # ''' \n","    A = LSTM_A(inputs, CELL_SIZE = 32) # 训练全部特征  (?, ?, 32)--> (?, 32)\n","    A1 = LSTM_A(part[0], 3, CELL_SIZE = 16) # x、y、z  (?, ?, 16)--> (?, 16)\n","    A2 = LSTM_A(part[1], 3, CELL_SIZE = 16) # xg、yg、zg\n","    A3 = LSTM_A(Concatenate()([part[2], part[3], part[4]]), 3, CELL_SIZE = 16) # mod、modg、g\n","    A4 = LSTM_A(Concatenate()([part[0], part[2]]), 4, CELL_SIZE = 32) # x、y、z、mod\n","    A5 = LSTM_A(Concatenate()([part[1], part[3]]), 4, CELL_SIZE = 32) # xg、yg、zg、modg\n","    \n","    x = Concatenate()([A,A1,A2,A3,A4,A5]) # (?, 32+16+16+16+32+32) --> (?, 144)\n","    x = BatchNormalization()(x)  \n","    x = Dropout(0.3)(x) \n","    x = Dense(256, activation='relu')(x)  \n","    X = BatchNormalization()(x) \n","    output1 = Dense(4, activation='softmax', name='4class')(X)   # 大类-字母\n","\n","    X = Dense(128, activation='relu')(X)\n","    x = Dropout(0.2)(x)\n","    X = BatchNormalization()(X)  \n","    output2 = Dense(7, activation='softmax', name='7class')(X)   # 大类-数字\n","    \n","    X = Dense(64, activation='relu')(X)\n","    x = Dropout(0.2)(x)\n","    X = BatchNormalization()(X)\n","    output3 = Dense(19, activation='softmax', name='19class')(X) # 小类\n","\n","    return Model([inputs], [output1,output2,output3])\n","\n","# graph_path = model_path + 'Multiloss.png'\n","# plot_model(LSTM_Model(), to_file = graph_path, show_shapes = True)  # 绘制模型图"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aAsEd93OFS-D","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1599057455448,"user_tz":-480,"elapsed":2203,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"63b24639-2c33-47f1-f30c-0042b8e0b289"},"source":["###################### 类别不平衡分析 ######################\n","mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n","        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n","        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n","        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n","        16: 'C_2', 17: 'C_5', 18: 'C_6'} \n","from sklearn.utils.class_weight import compute_class_weight \n","# 参考：https://www.cnblogs.com/qi-yuan-008/p/11992156.html  weight_ = n_samples / (n_classes * np.bincount(y)) \n","# https://blog.csdn.net/xlinsist/article/details/51346523  np.bincount(y)\n","# y_train_weight = compute_sample_weight(\"balanced\", train['behavior_id']) \n","classweights1=compute_class_weight(\"balanced\", ['A','B','C','D'], pd.read_csv(data_path+'sensor_train.csv')['behavior_id'].apply(lambda x:mapping[x][0]))\n","# weight --> array([0.74532854, 0.78667626, 0.86810794, 4.25154926])\n","classweights1=pd.DataFrame(classweights1)[0].to_dict() # [0]代表列名，由Series用.to_dict()方法得到字典。\n","print(classweights1)  \n","\n","classweights2=compute_class_weight(\"balanced\", list(range(7)), pd.read_csv(data_path+'sensor_train.csv')['behavior_id'].apply(lambda x:int(mapping[x][2])))\n","# weight --> array([1.75719532, 0.48154794, 0.96160228, 1.14122322, 2.42945672, 1.23675679, 0.8210787 ])\n","classweights2=pd.DataFrame(classweights2)[0].to_dict()\n","print(classweights2) \n","\n","# y_train_weight = compute_sample_weight(\"balanced\", train['behavior_id'])\n","classweights3=compute_class_weight(\"balanced\",np.array(range(19)), pd.read_csv(data_path+'sensor_train.csv')['behavior_id'])\n","classweights3=pd.DataFrame(classweights3)[0].to_dict() \n","print(classweights3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{0: 0.7453285438934641, 1: 0.7866762590992484, 2: 0.8681079382183908, 3: 4.251549256356949}\n","{0: 1.757195321956318, 1: 0.4815479398086302, 2: 0.9616022823865589, 3: 1.1412232173040497, 4: 2.4294567179182565, 5: 1.2367567913331454, 6: 0.8210787010495146}\n","{0: 1.6146639588513296, 1: 0.5658077637797579, 2: 0.9428223116223914, 3: 1.1544614165363905, 4: 0.895063001338305, 5: 1.120093850476494, 6: 0.5129293816036677, 7: 1.6147804233607677, 8: 1.1629774436090226, 9: 1.3287818013695207, 10: 1.639495846903968, 11: 0.8571276001942526, 12: 0.5209381219191084, 13: 1.3164363042146117, 14: 3.1705588145409553, 15: 0.8020390423628304, 16: 1.108447580802777, 17: 1.4649467209444893, 18: 1.12076674790857}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HB9qePPiFTAt","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1599059657601,"user_tz":-480,"elapsed":299377,"user":{"displayName":"余凯","photoUrl":"","userId":"13334016005065607071"}},"outputId":"e846e2e5-0f26-4166-c412-d73c706c31f1"},"source":["train = x\n","test = t\n","\n","fold_num = 5\n","proba_t = np.zeros((7500, 19))\n","proba_oof = np.zeros((7292, 19))\n","\n","oof_score = []\n","oof_comm = []\n","\n","from tensorflow.keras.losses import categorical_crossentropy\n","def custom_loss(y_true, y_pred):\n","  return categorical_crossentropy(y_true, y_pred, label_smoothing=0.05) \n","\n","# 两个输出    \n","mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n","        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n","        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n","        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n","        16: 'C_2', 17: 'C_5', 18: 'C_6'} \n","# 每一个大类输出 4 \n","new_mapping = {'A':0, 'B':1, 'C':2, 'D':3}\n","y_1 = to_categorical([new_mapping[mapping[x][0]] for x in y], num_classes=4) # y = train.groupby('fragment_id')['behavior_id'].min()\n","# 每一个大类输出 7\n","y_2 = to_categorical([mapping[x][2] for x in y], num_classes=7)\n","# 每一个小类的输出 19 \n","y_3 = to_categorical(y, num_classes=19)\n","\n","seeds = [42, 39, 17] ## bagging\n","for seed in seeds: \n","  kfold = StratifiedKFold(fold_num, random_state=seed, shuffle=True)\n","  for fold, (xx, yy) in enumerate(kfold.split(train, y)): ## stacking\n","      print(train.shape) # (7292, 60, 9)\n","      model = LSTM_Model() \n","      graph_path = model_path + 'Multiloss.png'\n","      plot_model(model, to_file = graph_path, show_shapes = True)  # 绘制模型图\n","      model.compile(loss=[custom_loss, custom_loss, custom_loss], ### 三个输入对应三个损失，分别所占权重为其对应的得分\n","              loss_weights=[3, 7, 21], # 指定标量系数以权衡不同模型输出的损失贡献。然后，将由模型最小化的损失值将是所有单个损失的“加权和” *，并由loss_weights系数加权。\n","              optimizer='Nadam', \n","              metrics=[\"acc\"]) \n","      plateau = ReduceLROnPlateau(monitor=\"val_19class_acc\",\n","                      verbose=1, \n","                      mode='max',\n","                      factor=0.7, \n","                      patience=18) \n","      early_stopping = EarlyStopping(monitor=\"val_19class_acc\",\n","                        verbose=1,\n","                        mode='max',\n","                        patience=60)  \n","      checkpoint = ModelCheckpoint(model_path + f'multiloss_fold{fold}.h5',   \n","                      monitor=\"val_19class_acc\",\n","                      verbose=0,\n","                      mode='max',\n","                      save_best_only=True)\n","  \n","      train_res = model.fit(train[xx], \n","                  [y_1[xx], y_2[xx], y_3[xx]],\n","                  epochs=400,\n","                  batch_size=32,\n","                  verbose=1,\n","                  shuffle=True,\n","                  validation_data=(train[yy], [y_1[yy], y_2[yy], y_3[yy]]),\n","                  callbacks=[plateau, early_stopping, checkpoint],\n","                  # class_weight=[classweights1, classweights2, classweights3]\n","                  ) \n","                  ## 针对类别不平衡问题设置类别权重\n","                  ## class_weight = 'auto' keras会自动设置class_weight让每类的sample对损失的贡献相等。\n","      # '''              \n","      # class_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）。\n","      # 该参数在处理非平衡的训练数据（某些类的训练样本数很少）时，可以使得损失函数对样本数不足的数据更加关注。\n","      # '''\n","\n","      model.load_weights(model_path + f'multiloss_fold{fold}.h5')\n","      proba_t += model.predict(test, verbose=0, batch_size=1024)[2] / fold_num / len(seeds)\n","      proba_oof[yy] += model.predict(train[yy], verbose=0, batch_size=1024)[2] / len(seeds)\n","\n","      oof_y = np.argmax(proba_oof[yy], axis=1)\n","      acc = round(accuracy_score(y[yy], oof_y), 3)\n","      print(acc) \n","      oof_score.append(acc)\n","      scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y[yy], oof_y)) / oof_y.shape[0]\n","      oof_comm.append(scores)   \n","      print(round(scores, 5))\n","\n","for index,i in enumerate(oof_comm):\n","  print(index,i,oof_score[index])\n","\n","oof_dict = { \"oof\":proba_oof,\n","        \"test\":proba_t,\n","        \"acc\":oof_comm } \n","import joblib \n","joblib.dump(oof_dict, model_path + \"LSTM.pkl\" % np.mean(oof_comm))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(7292, 60, 9)\n","Epoch 1/400\n","183/183 [==============================] - 19s 104ms/step - loss: 58.0062 - 4class_loss: 1.0260 - 7class_loss: 1.4908 - 19class_loss: 2.1187 - 4class_acc: 0.5949 - 7class_acc: 0.4581 - 19class_acc: 0.3415 - val_loss: 45.6178 - val_4class_loss: 0.7660 - val_7class_loss: 1.1527 - val_19class_loss: 1.6786 - val_4class_acc: 0.7108 - val_7class_acc: 0.5819 - val_19class_acc: 0.4907\n","Epoch 2/400\n","183/183 [==============================] - 14s 74ms/step - loss: 46.2977 - 4class_loss: 0.8043 - 7class_loss: 1.1924 - 19class_loss: 1.6923 - 4class_acc: 0.6856 - 7class_acc: 0.5849 - 19class_acc: 0.4795 - val_loss: 43.0583 - val_4class_loss: 0.7119 - val_7class_loss: 1.0792 - val_19class_loss: 1.5890 - val_4class_acc: 0.7347 - val_7class_acc: 0.6456 - val_19class_acc: 0.5093\n","Epoch 3/400\n","183/183 [==============================] - 14s 79ms/step - loss: 42.9359 - 4class_loss: 0.7432 - 7class_loss: 1.1161 - 19class_loss: 1.5664 - 4class_acc: 0.7115 - 7class_acc: 0.6124 - 19class_acc: 0.5183 - val_loss: 38.6807 - val_4class_loss: 0.6853 - val_7class_loss: 0.9847 - val_19class_loss: 1.4158 - val_4class_acc: 0.7498 - val_7class_acc: 0.6847 - val_19class_acc: 0.5819\n","Epoch 4/400\n","183/183 [==============================] - 13s 74ms/step - loss: 40.0240 - 4class_loss: 0.6908 - 7class_loss: 1.0503 - 19class_loss: 1.4571 - 4class_acc: 0.7521 - 7class_acc: 0.6503 - 19class_acc: 0.5580 - val_loss: 36.5935 - val_4class_loss: 0.6279 - val_7class_loss: 0.9412 - val_19class_loss: 1.3391 - val_4class_acc: 0.7848 - val_7class_acc: 0.6929 - val_19class_acc: 0.6018\n","Epoch 5/400\n","183/183 [==============================] - 12s 68ms/step - loss: 37.9994 - 4class_loss: 0.6767 - 7class_loss: 0.9918 - 19class_loss: 1.3822 - 4class_acc: 0.7557 - 7class_acc: 0.6678 - 19class_acc: 0.5885 - val_loss: 37.0493 - val_4class_loss: 0.6213 - val_7class_loss: 0.9485 - val_19class_loss: 1.3593 - val_4class_acc: 0.7855 - val_7class_acc: 0.6888 - val_19class_acc: 0.5860\n","Epoch 6/400\n","183/183 [==============================] - 13s 73ms/step - loss: 37.0568 - 4class_loss: 0.6552 - 7class_loss: 0.9809 - 19class_loss: 1.3441 - 4class_acc: 0.7653 - 7class_acc: 0.6796 - 19class_acc: 0.6074 - val_loss: 34.7947 - val_4class_loss: 0.6114 - val_7class_loss: 0.8921 - val_19class_loss: 1.2722 - val_4class_acc: 0.7882 - val_7class_acc: 0.7258 - val_19class_acc: 0.6374\n","Epoch 7/400\n","183/183 [==============================] - 12s 67ms/step - loss: 35.4196 - 4class_loss: 0.6420 - 7class_loss: 0.9348 - 19class_loss: 1.2833 - 4class_acc: 0.7732 - 7class_acc: 0.7050 - 19class_acc: 0.6254 - val_loss: 35.3095 - val_4class_loss: 0.5912 - val_7class_loss: 0.9203 - val_19class_loss: 1.2902 - val_4class_acc: 0.8122 - val_7class_acc: 0.7005 - val_19class_acc: 0.6223\n","Epoch 8/400\n","183/183 [==============================] - 12s 67ms/step - loss: 34.5770 - 4class_loss: 0.6280 - 7class_loss: 0.9064 - 19class_loss: 1.2547 - 4class_acc: 0.7773 - 7class_acc: 0.7125 - 19class_acc: 0.6321 - val_loss: 34.5558 - val_4class_loss: 0.6159 - val_7class_loss: 0.8807 - val_19class_loss: 1.2640 - val_4class_acc: 0.7800 - val_7class_acc: 0.7238 - val_19class_acc: 0.6313\n","Epoch 9/400\n","183/183 [==============================] - 13s 72ms/step - loss: 34.1123 - 4class_loss: 0.6201 - 7class_loss: 0.9052 - 19class_loss: 1.2341 - 4class_acc: 0.7878 - 7class_acc: 0.7113 - 19class_acc: 0.6396 - val_loss: 32.4312 - val_4class_loss: 0.5973 - val_7class_loss: 0.8340 - val_19class_loss: 1.1810 - val_4class_acc: 0.8081 - val_7class_acc: 0.7457 - val_19class_acc: 0.6737\n","Epoch 10/400\n","183/183 [==============================] - 12s 67ms/step - loss: 32.8721 - 4class_loss: 0.6079 - 7class_loss: 0.8725 - 19class_loss: 1.1877 - 4class_acc: 0.7948 - 7class_acc: 0.7269 - 19class_acc: 0.6619 - val_loss: 32.4351 - val_4class_loss: 0.5707 - val_7class_loss: 0.8323 - val_19class_loss: 1.1856 - val_4class_acc: 0.8211 - val_7class_acc: 0.7581 - val_19class_acc: 0.6600\n","Epoch 11/400\n","183/183 [==============================] - 13s 73ms/step - loss: 32.0655 - 4class_loss: 0.6065 - 7class_loss: 0.8502 - 19class_loss: 1.1569 - 4class_acc: 0.7975 - 7class_acc: 0.7404 - 19class_acc: 0.6755 - val_loss: 31.6805 - val_4class_loss: 0.5548 - val_7class_loss: 0.8215 - val_19class_loss: 1.1555 - val_4class_acc: 0.8355 - val_7class_acc: 0.7505 - val_19class_acc: 0.6751\n","Epoch 12/400\n","183/183 [==============================] - 12s 68ms/step - loss: 31.6043 - 4class_loss: 0.6019 - 7class_loss: 0.8437 - 19class_loss: 1.1377 - 4class_acc: 0.7996 - 7class_acc: 0.7447 - 19class_acc: 0.6772 - val_loss: 32.4101 - val_4class_loss: 0.5638 - val_7class_loss: 0.8335 - val_19class_loss: 1.1850 - val_4class_acc: 0.8129 - val_7class_acc: 0.7512 - val_19class_acc: 0.6559\n","Epoch 13/400\n","183/183 [==============================] - 13s 68ms/step - loss: 31.0097 - 4class_loss: 0.5917 - 7class_loss: 0.8241 - 19class_loss: 1.1174 - 4class_acc: 0.8023 - 7class_acc: 0.7435 - 19class_acc: 0.6880 - val_loss: 32.4259 - val_4class_loss: 0.5628 - val_7class_loss: 0.8392 - val_19class_loss: 1.1840 - val_4class_acc: 0.8191 - val_7class_acc: 0.7354 - val_19class_acc: 0.6703\n","Epoch 14/400\n","183/183 [==============================] - 12s 68ms/step - loss: 30.7613 - 4class_loss: 0.5916 - 7class_loss: 0.8169 - 19class_loss: 1.1080 - 4class_acc: 0.8066 - 7class_acc: 0.7511 - 19class_acc: 0.6864 - val_loss: 32.4911 - val_4class_loss: 0.5573 - val_7class_loss: 0.8395 - val_19class_loss: 1.1878 - val_4class_acc: 0.8348 - val_7class_acc: 0.7485 - val_19class_acc: 0.6676\n","Epoch 15/400\n","183/183 [==============================] - 13s 73ms/step - loss: 30.1826 - 4class_loss: 0.5902 - 7class_loss: 0.8009 - 19class_loss: 1.0860 - 4class_acc: 0.8095 - 7class_acc: 0.7622 - 19class_acc: 0.7041 - val_loss: 31.0791 - val_4class_loss: 0.5524 - val_7class_loss: 0.8072 - val_19class_loss: 1.1320 - val_4class_acc: 0.8341 - val_7class_acc: 0.7574 - val_19class_acc: 0.6964\n","Epoch 16/400\n","183/183 [==============================] - 13s 72ms/step - loss: 29.3916 - 4class_loss: 0.5768 - 7class_loss: 0.7838 - 19class_loss: 1.0559 - 4class_acc: 0.8178 - 7class_acc: 0.7677 - 19class_acc: 0.7122 - val_loss: 30.6885 - val_4class_loss: 0.5698 - val_7class_loss: 0.7855 - val_19class_loss: 1.1181 - val_4class_acc: 0.8156 - val_7class_acc: 0.7663 - val_19class_acc: 0.7005\n","Epoch 17/400\n","183/183 [==============================] - 13s 72ms/step - loss: 29.2834 - 4class_loss: 0.5728 - 7class_loss: 0.7822 - 19class_loss: 1.0519 - 4class_acc: 0.8183 - 7class_acc: 0.7703 - 19class_acc: 0.7142 - val_loss: 30.3748 - val_4class_loss: 0.5497 - val_7class_loss: 0.7926 - val_19class_loss: 1.1037 - val_4class_acc: 0.8266 - val_7class_acc: 0.7629 - val_19class_acc: 0.7046\n","Epoch 18/400\n","183/183 [==============================] - 13s 73ms/step - loss: 28.5212 - 4class_loss: 0.5725 - 7class_loss: 0.7659 - 19class_loss: 1.0211 - 4class_acc: 0.8250 - 7class_acc: 0.7734 - 19class_acc: 0.7228 - val_loss: 30.0624 - val_4class_loss: 0.5448 - val_7class_loss: 0.7978 - val_19class_loss: 1.0878 - val_4class_acc: 0.8403 - val_7class_acc: 0.7601 - val_19class_acc: 0.7135\n","Epoch 19/400\n","183/183 [==============================] - 13s 73ms/step - loss: 28.3600 - 4class_loss: 0.5619 - 7class_loss: 0.7601 - 19class_loss: 1.0168 - 4class_acc: 0.8272 - 7class_acc: 0.7790 - 19class_acc: 0.7276 - val_loss: 29.6897 - val_4class_loss: 0.5416 - val_7class_loss: 0.7704 - val_19class_loss: 1.0796 - val_4class_acc: 0.8458 - val_7class_acc: 0.7841 - val_19class_acc: 0.7156\n","Epoch 20/400\n","183/183 [==============================] - 13s 69ms/step - loss: 27.9565 - 4class_loss: 0.5624 - 7class_loss: 0.7515 - 19class_loss: 1.0004 - 4class_acc: 0.8220 - 7class_acc: 0.7891 - 19class_acc: 0.7363 - val_loss: 30.5261 - val_4class_loss: 0.5711 - val_7class_loss: 0.7781 - val_19class_loss: 1.1127 - val_4class_acc: 0.8204 - val_7class_acc: 0.7731 - val_19class_acc: 0.6957\n","Epoch 21/400\n","183/183 [==============================] - 12s 68ms/step - loss: 27.7691 - 4class_loss: 0.5551 - 7class_loss: 0.7458 - 19class_loss: 0.9944 - 4class_acc: 0.8280 - 7class_acc: 0.7852 - 19class_acc: 0.7362 - val_loss: 29.8377 - val_4class_loss: 0.5399 - val_7class_loss: 0.7769 - val_19class_loss: 1.0847 - val_4class_acc: 0.8410 - val_7class_acc: 0.7663 - val_19class_acc: 0.7108\n","Epoch 22/400\n","183/183 [==============================] - 12s 67ms/step - loss: 27.2960 - 4class_loss: 0.5561 - 7class_loss: 0.7327 - 19class_loss: 0.9761 - 4class_acc: 0.8227 - 7class_acc: 0.7975 - 19class_acc: 0.7454 - val_loss: 29.5729 - val_4class_loss: 0.5467 - val_7class_loss: 0.7750 - val_19class_loss: 1.0718 - val_4class_acc: 0.8341 - val_7class_acc: 0.7676 - val_19class_acc: 0.7142\n","Epoch 23/400\n","183/183 [==============================] - 13s 72ms/step - loss: 27.3699 - 4class_loss: 0.5612 - 7class_loss: 0.7270 - 19class_loss: 0.9808 - 4class_acc: 0.8256 - 7class_acc: 0.7944 - 19class_acc: 0.7427 - val_loss: 29.1424 - val_4class_loss: 0.5380 - val_7class_loss: 0.7606 - val_19class_loss: 1.0573 - val_4class_acc: 0.8376 - val_7class_acc: 0.7786 - val_19class_acc: 0.7341\n","Epoch 24/400\n","183/183 [==============================] - 12s 68ms/step - loss: 26.3895 - 4class_loss: 0.5508 - 7class_loss: 0.7089 - 19class_loss: 0.9417 - 4class_acc: 0.8323 - 7class_acc: 0.8063 - 19class_acc: 0.7662 - val_loss: 30.4529 - val_4class_loss: 0.5430 - val_7class_loss: 0.7831 - val_19class_loss: 1.1115 - val_4class_acc: 0.8396 - val_7class_acc: 0.7793 - val_19class_acc: 0.7258\n","Epoch 25/400\n","183/183 [==============================] - 13s 72ms/step - loss: 26.6704 - 4class_loss: 0.5510 - 7class_loss: 0.7132 - 19class_loss: 0.9536 - 4class_acc: 0.8313 - 7class_acc: 0.8047 - 19class_acc: 0.7578 - val_loss: 28.6067 - val_4class_loss: 0.5295 - val_7class_loss: 0.7448 - val_19class_loss: 1.0383 - val_4class_acc: 0.8540 - val_7class_acc: 0.7855 - val_19class_acc: 0.7389\n","Epoch 26/400\n","183/183 [==============================] - 12s 68ms/step - loss: 25.5905 - 4class_loss: 0.5416 - 7class_loss: 0.6850 - 19class_loss: 0.9129 - 4class_acc: 0.8363 - 7class_acc: 0.8174 - 19class_acc: 0.7713 - val_loss: 29.9822 - val_4class_loss: 0.5287 - val_7class_loss: 0.7704 - val_19class_loss: 1.0954 - val_4class_acc: 0.8554 - val_7class_acc: 0.7834 - val_19class_acc: 0.7272\n","Epoch 27/400\n","183/183 [==============================] - 12s 68ms/step - loss: 26.4299 - 4class_loss: 0.5449 - 7class_loss: 0.7091 - 19class_loss: 0.9443 - 4class_acc: 0.8308 - 7class_acc: 0.8049 - 19class_acc: 0.7610 - val_loss: 28.4632 - val_4class_loss: 0.5172 - val_7class_loss: 0.7414 - val_19class_loss: 1.0344 - val_4class_acc: 0.8478 - val_7class_acc: 0.7951 - val_19class_acc: 0.7368\n","Epoch 28/400\n","183/183 [==============================] - 12s 68ms/step - loss: 25.2777 - 4class_loss: 0.5350 - 7class_loss: 0.6808 - 19class_loss: 0.9003 - 4class_acc: 0.8412 - 7class_acc: 0.8159 - 19class_acc: 0.7792 - val_loss: 28.7694 - val_4class_loss: 0.5176 - val_7class_loss: 0.7436 - val_19class_loss: 1.0482 - val_4class_acc: 0.8478 - val_7class_acc: 0.7910 - val_19class_acc: 0.7334\n","Epoch 29/400\n","183/183 [==============================] - 13s 73ms/step - loss: 25.0965 - 4class_loss: 0.5315 - 7class_loss: 0.6776 - 19class_loss: 0.8933 - 4class_acc: 0.8442 - 7class_acc: 0.8238 - 19class_acc: 0.7770 - val_loss: 28.5525 - val_4class_loss: 0.5324 - val_7class_loss: 0.7304 - val_19class_loss: 1.0401 - val_4class_acc: 0.8430 - val_7class_acc: 0.7923 - val_19class_acc: 0.7402\n","Epoch 30/400\n","183/183 [==============================] - 13s 73ms/step - loss: 25.3552 - 4class_loss: 0.5295 - 7class_loss: 0.6808 - 19class_loss: 0.9048 - 4class_acc: 0.8421 - 7class_acc: 0.8162 - 19class_acc: 0.7735 - val_loss: 27.3549 - val_4class_loss: 0.5099 - val_7class_loss: 0.7178 - val_19class_loss: 0.9905 - val_4class_acc: 0.8520 - val_7class_acc: 0.8115 - val_19class_acc: 0.7560\n","Epoch 31/400\n","183/183 [==============================] - 13s 68ms/step - loss: 25.1400 - 4class_loss: 0.5283 - 7class_loss: 0.6748 - 19class_loss: 0.8968 - 4class_acc: 0.8397 - 7class_acc: 0.8203 - 19class_acc: 0.7749 - val_loss: 27.6366 - val_4class_loss: 0.5049 - val_7class_loss: 0.7280 - val_19class_loss: 1.0012 - val_4class_acc: 0.8609 - val_7class_acc: 0.7896 - val_19class_acc: 0.7402\n","Epoch 32/400\n","183/183 [==============================] - 13s 69ms/step - loss: 24.6861 - 4class_loss: 0.5244 - 7class_loss: 0.6634 - 19class_loss: 0.8795 - 4class_acc: 0.8490 - 7class_acc: 0.8316 - 19class_acc: 0.7920 - val_loss: 28.1312 - val_4class_loss: 0.5079 - val_7class_loss: 0.7462 - val_19class_loss: 1.0183 - val_4class_acc: 0.8547 - val_7class_acc: 0.7889 - val_19class_acc: 0.7443\n","Epoch 33/400\n","183/183 [==============================] - 13s 68ms/step - loss: 24.5060 - 4class_loss: 0.5299 - 7class_loss: 0.6624 - 19class_loss: 0.8705 - 4class_acc: 0.8471 - 7class_acc: 0.8275 - 19class_acc: 0.7862 - val_loss: 28.3395 - val_4class_loss: 0.5089 - val_7class_loss: 0.7274 - val_19class_loss: 1.0343 - val_4class_acc: 0.8622 - val_7class_acc: 0.7896 - val_19class_acc: 0.7430\n","Epoch 34/400\n","183/183 [==============================] - 13s 73ms/step - loss: 24.3606 - 4class_loss: 0.5283 - 7class_loss: 0.6565 - 19class_loss: 0.8657 - 4class_acc: 0.8438 - 7class_acc: 0.8322 - 19class_acc: 0.7905 - val_loss: 26.9957 - val_4class_loss: 0.5026 - val_7class_loss: 0.7154 - val_19class_loss: 0.9753 - val_4class_acc: 0.8581 - val_7class_acc: 0.8060 - val_19class_acc: 0.7676\n","Epoch 35/400\n","183/183 [==============================] - 13s 69ms/step - loss: 23.8595 - 4class_loss: 0.5210 - 7class_loss: 0.6440 - 19class_loss: 0.8471 - 4class_acc: 0.8510 - 7class_acc: 0.8322 - 19class_acc: 0.7956 - val_loss: 28.0087 - val_4class_loss: 0.5119 - val_7class_loss: 0.7200 - val_19class_loss: 1.0206 - val_4class_acc: 0.8615 - val_7class_acc: 0.8047 - val_19class_acc: 0.7526\n","Epoch 36/400\n","183/183 [==============================] - 13s 69ms/step - loss: 23.8445 - 4class_loss: 0.5170 - 7class_loss: 0.6425 - 19class_loss: 0.8474 - 4class_acc: 0.8503 - 7class_acc: 0.8395 - 19class_acc: 0.8020 - val_loss: 27.7277 - val_4class_loss: 0.5247 - val_7class_loss: 0.7139 - val_19class_loss: 1.0075 - val_4class_acc: 0.8615 - val_7class_acc: 0.8012 - val_19class_acc: 0.7533\n","Epoch 37/400\n","183/183 [==============================] - 13s 69ms/step - loss: 23.5688 - 4class_loss: 0.5226 - 7class_loss: 0.6314 - 19class_loss: 0.8372 - 4class_acc: 0.8502 - 7class_acc: 0.8380 - 19class_acc: 0.7999 - val_loss: 28.1601 - val_4class_loss: 0.5172 - val_7class_loss: 0.7242 - val_19class_loss: 1.0257 - val_4class_acc: 0.8547 - val_7class_acc: 0.7937 - val_19class_acc: 0.7519\n","Epoch 38/400\n","183/183 [==============================] - 13s 68ms/step - loss: 23.4617 - 4class_loss: 0.5125 - 7class_loss: 0.6357 - 19class_loss: 0.8321 - 4class_acc: 0.8517 - 7class_acc: 0.8435 - 19class_acc: 0.8034 - val_loss: 28.0244 - val_4class_loss: 0.5200 - val_7class_loss: 0.7271 - val_19class_loss: 1.0178 - val_4class_acc: 0.8533 - val_7class_acc: 0.8088 - val_19class_acc: 0.7581\n","Epoch 39/400\n","183/183 [==============================] - 13s 73ms/step - loss: 23.5740 - 4class_loss: 0.5156 - 7class_loss: 0.6322 - 19class_loss: 0.8382 - 4class_acc: 0.8541 - 7class_acc: 0.8438 - 19class_acc: 0.8003 - val_loss: 26.7914 - val_4class_loss: 0.4997 - val_7class_loss: 0.6983 - val_19class_loss: 0.9716 - val_4class_acc: 0.8629 - val_7class_acc: 0.8081 - val_19class_acc: 0.7718\n","Epoch 40/400\n","183/183 [==============================] - 13s 68ms/step - loss: 23.2832 - 4class_loss: 0.5092 - 7class_loss: 0.6270 - 19class_loss: 0.8270 - 4class_acc: 0.8543 - 7class_acc: 0.8416 - 19class_acc: 0.8015 - val_loss: 26.6186 - val_4class_loss: 0.4955 - val_7class_loss: 0.6976 - val_19class_loss: 0.9642 - val_4class_acc: 0.8691 - val_7class_acc: 0.8081 - val_19class_acc: 0.7683\n","Epoch 41/400\n","183/183 [==============================] - 13s 73ms/step - loss: 23.2577 - 4class_loss: 0.5106 - 7class_loss: 0.6270 - 19class_loss: 0.8256 - 4class_acc: 0.8591 - 7class_acc: 0.8424 - 19class_acc: 0.8099 - val_loss: 25.8566 - val_4class_loss: 0.5011 - val_7class_loss: 0.6861 - val_19class_loss: 0.9310 - val_4class_acc: 0.8602 - val_7class_acc: 0.8184 - val_19class_acc: 0.7834\n","Epoch 42/400\n","183/183 [==============================] - 13s 68ms/step - loss: 22.7845 - 4class_loss: 0.5131 - 7class_loss: 0.6158 - 19class_loss: 0.8064 - 4class_acc: 0.8527 - 7class_acc: 0.8466 - 19class_acc: 0.8174 - val_loss: 27.6330 - val_4class_loss: 0.4998 - val_7class_loss: 0.7174 - val_19class_loss: 1.0053 - val_4class_acc: 0.8643 - val_7class_acc: 0.8129 - val_19class_acc: 0.7711\n","Epoch 43/400\n","183/183 [==============================] - 13s 72ms/step - loss: 22.8537 - 4class_loss: 0.5029 - 7class_loss: 0.6183 - 19class_loss: 0.8103 - 4class_acc: 0.8546 - 7class_acc: 0.8471 - 19class_acc: 0.8133 - val_loss: 26.2857 - val_4class_loss: 0.4932 - val_7class_loss: 0.6885 - val_19class_loss: 0.9518 - val_4class_acc: 0.8698 - val_7class_acc: 0.8204 - val_19class_acc: 0.7889\n","Epoch 44/400\n","183/183 [==============================] - 12s 68ms/step - loss: 22.6816 - 4class_loss: 0.4970 - 7class_loss: 0.6115 - 19class_loss: 0.8052 - 4class_acc: 0.8652 - 7class_acc: 0.8508 - 19class_acc: 0.8179 - val_loss: 26.8990 - val_4class_loss: 0.4963 - val_7class_loss: 0.7078 - val_19class_loss: 0.9741 - val_4class_acc: 0.8657 - val_7class_acc: 0.8081 - val_19class_acc: 0.7656\n","Epoch 45/400\n","183/183 [==============================] - 12s 68ms/step - loss: 22.6980 - 4class_loss: 0.5030 - 7class_loss: 0.6136 - 19class_loss: 0.8045 - 4class_acc: 0.8611 - 7class_acc: 0.8502 - 19class_acc: 0.8140 - val_loss: 26.6724 - val_4class_loss: 0.5030 - val_7class_loss: 0.6919 - val_19class_loss: 0.9676 - val_4class_acc: 0.8609 - val_7class_acc: 0.8163 - val_19class_acc: 0.7779\n","Epoch 46/400\n","183/183 [==============================] - 13s 69ms/step - loss: 22.2578 - 4class_loss: 0.4939 - 7class_loss: 0.6065 - 19class_loss: 0.7872 - 4class_acc: 0.8620 - 7class_acc: 0.8488 - 19class_acc: 0.8198 - val_loss: 26.8671 - val_4class_loss: 0.5047 - val_7class_loss: 0.6956 - val_19class_loss: 0.9754 - val_4class_acc: 0.8526 - val_7class_acc: 0.8218 - val_19class_acc: 0.7567\n","Epoch 47/400\n","183/183 [==============================] - 12s 68ms/step - loss: 22.4415 - 4class_loss: 0.5050 - 7class_loss: 0.6097 - 19class_loss: 0.7933 - 4class_acc: 0.8606 - 7class_acc: 0.8507 - 19class_acc: 0.8200 - val_loss: 28.0938 - val_4class_loss: 0.4996 - val_7class_loss: 0.7452 - val_19class_loss: 1.0180 - val_4class_acc: 0.8568 - val_7class_acc: 0.8074 - val_19class_acc: 0.7567\n","Epoch 48/400\n","183/183 [==============================] - 13s 68ms/step - loss: 22.1484 - 4class_loss: 0.4997 - 7class_loss: 0.5987 - 19class_loss: 0.7837 - 4class_acc: 0.8594 - 7class_acc: 0.8565 - 19class_acc: 0.8274 - val_loss: 26.8788 - val_4class_loss: 0.4919 - val_7class_loss: 0.7149 - val_19class_loss: 0.9714 - val_4class_acc: 0.8643 - val_7class_acc: 0.8101 - val_19class_acc: 0.7793\n","Epoch 49/400\n","183/183 [==============================] - 13s 69ms/step - loss: 22.0905 - 4class_loss: 0.4908 - 7class_loss: 0.5991 - 19class_loss: 0.7821 - 4class_acc: 0.8627 - 7class_acc: 0.8486 - 19class_acc: 0.8193 - val_loss: 26.0964 - val_4class_loss: 0.5033 - val_7class_loss: 0.6805 - val_19class_loss: 0.9439 - val_4class_acc: 0.8622 - val_7class_acc: 0.8239 - val_19class_acc: 0.7868\n","Epoch 50/400\n","183/183 [==============================] - 13s 68ms/step - loss: 21.9455 - 4class_loss: 0.4926 - 7class_loss: 0.5931 - 19class_loss: 0.7770 - 4class_acc: 0.8630 - 7class_acc: 0.8546 - 19class_acc: 0.8256 - val_loss: 26.5258 - val_4class_loss: 0.5080 - val_7class_loss: 0.7006 - val_19class_loss: 0.9570 - val_4class_acc: 0.8520 - val_7class_acc: 0.8115 - val_19class_acc: 0.7738\n","Epoch 51/400\n","183/183 [==============================] - 12s 68ms/step - loss: 21.7431 - 4class_loss: 0.4924 - 7class_loss: 0.5843 - 19class_loss: 0.7703 - 4class_acc: 0.8651 - 7class_acc: 0.8582 - 19class_acc: 0.8316 - val_loss: 27.0298 - val_4class_loss: 0.4960 - val_7class_loss: 0.7001 - val_19class_loss: 0.9829 - val_4class_acc: 0.8602 - val_7class_acc: 0.8239 - val_19class_acc: 0.7820\n","Epoch 52/400\n","183/183 [==============================] - 13s 68ms/step - loss: 21.5104 - 4class_loss: 0.4856 - 7class_loss: 0.5836 - 19class_loss: 0.7604 - 4class_acc: 0.8712 - 7class_acc: 0.8634 - 19class_acc: 0.8332 - val_loss: 27.0743 - val_4class_loss: 0.5048 - val_7class_loss: 0.7007 - val_19class_loss: 0.9836 - val_4class_acc: 0.8622 - val_7class_acc: 0.8184 - val_19class_acc: 0.7642\n","Epoch 53/400\n","183/183 [==============================] - 12s 68ms/step - loss: 21.4589 - 4class_loss: 0.4903 - 7class_loss: 0.5820 - 19class_loss: 0.7578 - 4class_acc: 0.8664 - 7class_acc: 0.8659 - 19class_acc: 0.8356 - val_loss: 26.5419 - val_4class_loss: 0.4871 - val_7class_loss: 0.6923 - val_19class_loss: 0.9636 - val_4class_acc: 0.8677 - val_7class_acc: 0.8197 - val_19class_acc: 0.7786\n","Epoch 54/400\n","183/183 [==============================] - 12s 68ms/step - loss: 21.2516 - 4class_loss: 0.4866 - 7class_loss: 0.5810 - 19class_loss: 0.7488 - 4class_acc: 0.8688 - 7class_acc: 0.8676 - 19class_acc: 0.8436 - val_loss: 27.0174 - val_4class_loss: 0.4971 - val_7class_loss: 0.7196 - val_19class_loss: 0.9757 - val_4class_acc: 0.8670 - val_7class_acc: 0.8122 - val_19class_acc: 0.7814\n","Epoch 55/400\n","183/183 [==============================] - 12s 68ms/step - loss: 21.4677 - 4class_loss: 0.4854 - 7class_loss: 0.5874 - 19class_loss: 0.7571 - 4class_acc: 0.8694 - 7class_acc: 0.8610 - 19class_acc: 0.8359 - val_loss: 26.8874 - val_4class_loss: 0.4862 - val_7class_loss: 0.7011 - val_19class_loss: 0.9772 - val_4class_acc: 0.8732 - val_7class_acc: 0.8191 - val_19class_acc: 0.7862\n","Epoch 56/400\n","183/183 [==============================] - 12s 67ms/step - loss: 21.0067 - 4class_loss: 0.4787 - 7class_loss: 0.5724 - 19class_loss: 0.7411 - 4class_acc: 0.8762 - 7class_acc: 0.8608 - 19class_acc: 0.8361 - val_loss: 27.0834 - val_4class_loss: 0.4929 - val_7class_loss: 0.7109 - val_19class_loss: 0.9823 - val_4class_acc: 0.8636 - val_7class_acc: 0.8225 - val_19class_acc: 0.7752\n","Epoch 57/400\n","183/183 [==============================] - 12s 68ms/step - loss: 20.7152 - 4class_loss: 0.4792 - 7class_loss: 0.5644 - 19class_loss: 0.7299 - 4class_acc: 0.8757 - 7class_acc: 0.8702 - 19class_acc: 0.8484 - val_loss: 27.8423 - val_4class_loss: 0.4876 - val_7class_loss: 0.7356 - val_19class_loss: 1.0110 - val_4class_acc: 0.8705 - val_7class_acc: 0.8033 - val_19class_acc: 0.7656\n","Epoch 58/400\n","183/183 [==============================] - 14s 75ms/step - loss: 20.7351 - 4class_loss: 0.4784 - 7class_loss: 0.5660 - 19class_loss: 0.7304 - 4class_acc: 0.8773 - 7class_acc: 0.8702 - 19class_acc: 0.8474 - val_loss: 26.5882 - val_4class_loss: 0.4825 - val_7class_loss: 0.6956 - val_19class_loss: 0.9653 - val_4class_acc: 0.8739 - val_7class_acc: 0.8273 - val_19class_acc: 0.7882\n","Epoch 59/400\n","183/183 [==============================] - 13s 69ms/step - loss: 20.7639 - 4class_loss: 0.4726 - 7class_loss: 0.5743 - 19class_loss: 0.7298 - 4class_acc: 0.8793 - 7class_acc: 0.8664 - 19class_acc: 0.8455 - val_loss: 26.5578 - val_4class_loss: 0.4778 - val_7class_loss: 0.7059 - val_19class_loss: 0.9611 - val_4class_acc: 0.8780 - val_7class_acc: 0.8245 - val_19class_acc: 0.7807\n","Epoch 60/400\n","183/183 [==============================] - 13s 70ms/step - loss: 20.5772 - 4class_loss: 0.4761 - 7class_loss: 0.5594 - 19class_loss: 0.7254 - 4class_acc: 0.8755 - 7class_acc: 0.8748 - 19class_acc: 0.8481 - val_loss: 27.3172 - val_4class_loss: 0.4860 - val_7class_loss: 0.7145 - val_19class_loss: 0.9932 - val_4class_acc: 0.8677 - val_7class_acc: 0.8033 - val_19class_acc: 0.7820\n","Epoch 61/400\n","183/183 [==============================] - 14s 74ms/step - loss: 20.6509 - 4class_loss: 0.4760 - 7class_loss: 0.5602 - 19class_loss: 0.7286 - 4class_acc: 0.8781 - 7class_acc: 0.8755 - 19class_acc: 0.8496 - val_loss: 26.3887 - val_4class_loss: 0.4805 - val_7class_loss: 0.7016 - val_19class_loss: 0.9541 - val_4class_acc: 0.8794 - val_7class_acc: 0.8239 - val_19class_acc: 0.7896\n","Epoch 62/400\n","183/183 [==============================] - 13s 70ms/step - loss: 20.3599 - 4class_loss: 0.4826 - 7class_loss: 0.5515 - 19class_loss: 0.7168 - 4class_acc: 0.8723 - 7class_acc: 0.8776 - 19class_acc: 0.8524 - val_loss: 26.5431 - val_4class_loss: 0.4883 - val_7class_loss: 0.6930 - val_19class_loss: 0.9632 - val_4class_acc: 0.8801 - val_7class_acc: 0.8259 - val_19class_acc: 0.7848\n","Epoch 63/400\n","183/183 [==============================] - 14s 74ms/step - loss: 20.0232 - 4class_loss: 0.4709 - 7class_loss: 0.5475 - 19class_loss: 0.7037 - 4class_acc: 0.8803 - 7class_acc: 0.8807 - 19class_acc: 0.8616 - val_loss: 26.6468 - val_4class_loss: 0.4797 - val_7class_loss: 0.6978 - val_19class_loss: 0.9678 - val_4class_acc: 0.8773 - val_7class_acc: 0.8334 - val_19class_acc: 0.7944\n","Epoch 64/400\n","183/183 [==============================] - 13s 69ms/step - loss: 19.9618 - 4class_loss: 0.4594 - 7class_loss: 0.5485 - 19class_loss: 0.7021 - 4class_acc: 0.8917 - 7class_acc: 0.8817 - 19class_acc: 0.8620 - val_loss: 26.7249 - val_4class_loss: 0.4940 - val_7class_loss: 0.6913 - val_19class_loss: 0.9716 - val_4class_acc: 0.8684 - val_7class_acc: 0.8417 - val_19class_acc: 0.7944\n","Epoch 65/400\n","183/183 [==============================] - 13s 70ms/step - loss: 20.0034 - 4class_loss: 0.4662 - 7class_loss: 0.5487 - 19class_loss: 0.7030 - 4class_acc: 0.8805 - 7class_acc: 0.8810 - 19class_acc: 0.8562 - val_loss: 26.8064 - val_4class_loss: 0.4970 - val_7class_loss: 0.7046 - val_19class_loss: 0.9706 - val_4class_acc: 0.8725 - val_7class_acc: 0.8204 - val_19class_acc: 0.7903\n","Epoch 66/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.9627 - 4class_loss: 0.4599 - 7class_loss: 0.5526 - 19class_loss: 0.7007 - 4class_acc: 0.8841 - 7class_acc: 0.8786 - 19class_acc: 0.8634 - val_loss: 26.7168 - val_4class_loss: 0.4853 - val_7class_loss: 0.7014 - val_19class_loss: 0.9691 - val_4class_acc: 0.8718 - val_7class_acc: 0.8259 - val_19class_acc: 0.7930\n","Epoch 67/400\n","183/183 [==============================] - 14s 74ms/step - loss: 20.0990 - 4class_loss: 0.4734 - 7class_loss: 0.5511 - 19class_loss: 0.7058 - 4class_acc: 0.8817 - 7class_acc: 0.8810 - 19class_acc: 0.8601 - val_loss: 26.3803 - val_4class_loss: 0.4812 - val_7class_loss: 0.6941 - val_19class_loss: 0.9561 - val_4class_acc: 0.8780 - val_7class_acc: 0.8259 - val_19class_acc: 0.7958\n","Epoch 68/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.6306 - 4class_loss: 0.4691 - 7class_loss: 0.5389 - 19class_loss: 0.6881 - 4class_acc: 0.8809 - 7class_acc: 0.8845 - 19class_acc: 0.8668 - val_loss: 27.0916 - val_4class_loss: 0.4903 - val_7class_loss: 0.7136 - val_19class_loss: 0.9822 - val_4class_acc: 0.8821 - val_7class_acc: 0.8211 - val_19class_acc: 0.7834\n","Epoch 69/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.6047 - 4class_loss: 0.4699 - 7class_loss: 0.5370 - 19class_loss: 0.6874 - 4class_acc: 0.8791 - 7class_acc: 0.8893 - 19class_acc: 0.8682 - val_loss: 27.4153 - val_4class_loss: 0.4867 - val_7class_loss: 0.7179 - val_19class_loss: 0.9967 - val_4class_acc: 0.8801 - val_7class_acc: 0.8225 - val_19class_acc: 0.7848\n","Epoch 70/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.3014 - 4class_loss: 0.4596 - 7class_loss: 0.5328 - 19class_loss: 0.6759 - 4class_acc: 0.8886 - 7class_acc: 0.8913 - 19class_acc: 0.8733 - val_loss: 27.4954 - val_4class_loss: 0.4792 - val_7class_loss: 0.7209 - val_19class_loss: 1.0005 - val_4class_acc: 0.8746 - val_7class_acc: 0.8143 - val_19class_acc: 0.7814\n","Epoch 71/400\n","183/183 [==============================] - 13s 74ms/step - loss: 19.9502 - 4class_loss: 0.4674 - 7class_loss: 0.5510 - 19class_loss: 0.6996 - 4class_acc: 0.8867 - 7class_acc: 0.8798 - 19class_acc: 0.8656 - val_loss: 26.5514 - val_4class_loss: 0.4877 - val_7class_loss: 0.7004 - val_19class_loss: 0.9612 - val_4class_acc: 0.8821 - val_7class_acc: 0.8307 - val_19class_acc: 0.7999\n","Epoch 72/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.7276 - 4class_loss: 0.4662 - 7class_loss: 0.5420 - 19class_loss: 0.6921 - 4class_acc: 0.8831 - 7class_acc: 0.8863 - 19class_acc: 0.8680 - val_loss: 26.7452 - val_4class_loss: 0.4826 - val_7class_loss: 0.7029 - val_19class_loss: 0.9703 - val_4class_acc: 0.8705 - val_7class_acc: 0.8177 - val_19class_acc: 0.7882\n","Epoch 73/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.2804 - 4class_loss: 0.4567 - 7class_loss: 0.5289 - 19class_loss: 0.6766 - 4class_acc: 0.8899 - 7class_acc: 0.8915 - 19class_acc: 0.8687 - val_loss: 27.2166 - val_4class_loss: 0.4865 - val_7class_loss: 0.7166 - val_19class_loss: 0.9877 - val_4class_acc: 0.8698 - val_7class_acc: 0.8156 - val_19class_acc: 0.7862\n","Epoch 74/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.5545 - 4class_loss: 0.4700 - 7class_loss: 0.5316 - 19class_loss: 0.6868 - 4class_acc: 0.8795 - 7class_acc: 0.8894 - 19class_acc: 0.8675 - val_loss: 27.4926 - val_4class_loss: 0.4825 - val_7class_loss: 0.7274 - val_19class_loss: 0.9978 - val_4class_acc: 0.8801 - val_7class_acc: 0.8225 - val_19class_acc: 0.7834\n","Epoch 75/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.3866 - 4class_loss: 0.4540 - 7class_loss: 0.5339 - 19class_loss: 0.6803 - 4class_acc: 0.8882 - 7class_acc: 0.8863 - 19class_acc: 0.8670 - val_loss: 26.9335 - val_4class_loss: 0.4915 - val_7class_loss: 0.7077 - val_19class_loss: 0.9764 - val_4class_acc: 0.8739 - val_7class_acc: 0.8245 - val_19class_acc: 0.7903\n","Epoch 76/400\n","183/183 [==============================] - 14s 74ms/step - loss: 19.1400 - 4class_loss: 0.4578 - 7class_loss: 0.5253 - 19class_loss: 0.6709 - 4class_acc: 0.8898 - 7class_acc: 0.8958 - 19class_acc: 0.8714 - val_loss: 26.0385 - val_4class_loss: 0.4738 - val_7class_loss: 0.6843 - val_19class_loss: 0.9441 - val_4class_acc: 0.8794 - val_7class_acc: 0.8465 - val_19class_acc: 0.8040\n","Epoch 77/400\n","183/183 [==============================] - 13s 70ms/step - loss: 19.0871 - 4class_loss: 0.4525 - 7class_loss: 0.5228 - 19class_loss: 0.6700 - 4class_acc: 0.8923 - 7class_acc: 0.8999 - 19class_acc: 0.8755 - val_loss: 26.5316 - val_4class_loss: 0.4755 - val_7class_loss: 0.7045 - val_19class_loss: 0.9606 - val_4class_acc: 0.8814 - val_7class_acc: 0.8273 - val_19class_acc: 0.7930\n","Epoch 78/400\n","183/183 [==============================] - 13s 69ms/step - loss: 18.9180 - 4class_loss: 0.4571 - 7class_loss: 0.5166 - 19class_loss: 0.6634 - 4class_acc: 0.8884 - 7class_acc: 0.8966 - 19class_acc: 0.8748 - val_loss: 26.4226 - val_4class_loss: 0.4724 - val_7class_loss: 0.7086 - val_19class_loss: 0.9545 - val_4class_acc: 0.8780 - val_7class_acc: 0.8245 - val_19class_acc: 0.7951\n","Epoch 79/400\n","183/183 [==============================] - 13s 69ms/step - loss: 18.9352 - 4class_loss: 0.4560 - 7class_loss: 0.5162 - 19class_loss: 0.6645 - 4class_acc: 0.8881 - 7class_acc: 0.8949 - 19class_acc: 0.8755 - val_loss: 26.4449 - val_4class_loss: 0.4729 - val_7class_loss: 0.7108 - val_19class_loss: 0.9548 - val_4class_acc: 0.8842 - val_7class_acc: 0.8286 - val_19class_acc: 0.7985\n","Epoch 80/400\n","183/183 [==============================] - 13s 69ms/step - loss: 19.0305 - 4class_loss: 0.4621 - 7class_loss: 0.5209 - 19class_loss: 0.6666 - 4class_acc: 0.8874 - 7class_acc: 0.8970 - 19class_acc: 0.8798 - val_loss: 26.8608 - val_4class_loss: 0.4625 - val_7class_loss: 0.7098 - val_19class_loss: 0.9764 - val_4class_acc: 0.8849 - val_7class_acc: 0.8321 - val_19class_acc: 0.7916\n","Epoch 81/400\n","164/183 [=========================>....] - ETA: 1s - loss: 18.6389 - 4class_loss: 0.4532 - 7class_loss: 0.5115 - 19class_loss: 0.6523 - 4class_acc: 0.8853 - 7class_acc: 0.9019 - 19class_acc: 0.8857"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-3dc636307df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                   \u001b[0;31m# class_weight=[classweights1, classweights2, classweights3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                   ) \n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"PMbXLNAMFTFy"},"source":["# import seaborn as sns\n","# import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","\n","def acc_combo(y, y_pred):\n","    # 数值ID与行为编码的对应关系\n","    mapping = { 0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n","                4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n","                8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n","                12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n","                16: 'C_2', 17: 'C_5', 18: 'C_6'}\n","    # 将行为ID转为编码\n","    code_y, code_y_pred = mapping[y], mapping[y_pred]\n","    if code_y == code_y_pred: #编码完全相同得分1.0\n","        return 1.0\n","    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n","        return 1.0/7\n","    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n","        return 1.0/3\n","    else:\n","        return 0.0\n","\n","train_y = y\n","labels = np.argmax(proba_t, axis=1) # 测试集预测标签\n","oof_y = np.argmax(proba_oof, axis=1) # 训练集验证标签\n","print(round(accuracy_score(train_y, oof_y), 5))  \n","scores = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n","print(round(scores, 5)) \n","\n","sub = pd.read_csv(data_path + 'submit.csv')\n","sub['behavior_id'] = labels\n","\n","sub.to_csv(data_path + 'lstm%.5f.csv' % scores, index=False)\n","sub.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DroiCmZHdDsd"},"source":["vc = pd.Series(train_y).value_counts().sort_index() # 按索引从0到18排列\n","sns.barplot(vc.index, vc.values) # vc.values 由Series-->array\n","plt.show()\n","\n","vc = pd.Series(oof_y).value_counts().sort_index()\n","sns.barplot(vc.index, vc.values)\n","plt.show()\n","\n","vc = sub['behavior_id'].value_counts().sort_index()\n","sns.barplot(vc.index, vc.values)\n","plt.show()"],"execution_count":null,"outputs":[]}]}